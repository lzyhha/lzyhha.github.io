<head>
    <title>ZhongYu Li</title>
    <meta name="author" content="ZhongYu Li">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="ZhongYu Li">
	<meta property="og:description" content="PhD student, Nankai University">
    <meta property="og:image" content="assets/me.jpeg">
	<meta property="og:url" content="https://lzyhha.github.io/">
	<!-- <meta name="twitter:card" content="summary_large_image"> -->
    <!-- <link rel="apple-touch-icon" href="/assets/paper_teaser/ucsd-logo.png"> -->
    <!-- <link rel="icon" type="image/png" href="/assets/paper_teaser/ucsd-logo.png"> -->
    <!-- <link rel="manifest" href="/assets/paper_teaser/site.webmanifest"> -->
    <link rel="stylesheet" href="css/style.css">
</head>

<!-- <div class="navbar">
    <a href="#about">About</a>
    <a href="#news">News</a>
    <a href="#publications">Publications</a>
    <a href="#experiences">Experiences</a>
    <a href="#contact">Contact</a>
</div> -->

<div class="header noselect" id="about">
    <div class="content row">
        <div class="header-profile-picture" style="background-image: url(assets/me.jpg);"></div>
        <div class="header-text">
            <div class="header-name">
                <h1>Zhong-Yu Li</h1>
            </div>
            <div class="header-subtitle">
                PhD student, Nankai University
            </div>
            <div class="header-links">
                <a class="btn" href="#contact">Email</a> /
                <a class="btn" href="https://scholar.google.com/citations?user=g6WHXrgAAAAJ">Google Scholar</a> /
                <a class="btn" href="https://github.com/lzyhha">GitHub</a>
            </div>
            <div>
                <p>
                    Zhong-Yu Li is a PhD student at Nankai University, advised by Prof. <a href="https://mmcheng.net/">Ming-Ming Cheng</a>. 
                    His research interests include computer vision and deep learning, 
                    previously focusing on visual generation and representative learning.
                </p>
            </div>
        </div>
    </div>
</div>

<div class="content" style="padding-bottom: 64px;">

    <!-- Publications -->
    <div id="publications" class="noselect">
        <h2 class="noselect">Selected Publications <span style="font-size: 12px; color: #666;">(click to sort: 
            <a class="btn active" id="sort-author" onclick="sortPublications('author')">first author</a> / 
            <a class="btn" id="sort-date" onclick="sortPublications('date')">date</a>)</span></h2>
        <p>* Equal contribution. # Corresponding author. Representative papers are <span style="background-color: #fff8df">highlighted</span>.</p>
        <div class="highlight publication row clearfix" data-author="Zhong-Yu Li" data-date="202504">
            <div class="row-media" style="height: 120px; background-image: url(assets/paper_teaser/visualcloze_teaser.jpg);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://visualcloze.github.io/">VisualCloze : A Universal Image Generation Framework via Visual In-Context Learning</a><br/>
                <p>🔥 Support a wide range of in-domain tasks, and <span style="color:red;">generalize to unseen tasks.</span></p> 
                <span class="bold">Zhong-Yu Li*</span>, Ruoyi Du*, Juncheng Yan, Le Zhuo, Zhen Li#, Peng Gao, Zhanyu Ma, Ming-Ming Cheng#. <br/>
                <span class="italic">arxiv</span>, 2025 <br/>
                <a class="btn btn-red" href="https://visualcloze.github.io/">project</a> / 
                <a class="btn btn-red" href="https://huggingface.co/spaces/VisualCloze/VisualCloze/">online demo</a> /
                <a class="btn btn-red" href="https://arxiv.org/abs/2504.07960">paper</a> / 
                <a class="btn btn-red" href="https://github.com/lzyhha/VisualCloze">github</a> /
                <a class="btn btn-red" href="bibtex/li2025visualcloze.html">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix" data-author="Zhong-Yu Li" data-date="202503">
            <div class="row-media" style="height: 120px; background-image: url(assets/paper_teaser/aodraw_teaser.jpg);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2411.15678">Towards RAW Object Detection in Diverse Conditions</a><br/>
                <span class="bold">Zhong-Yu Li</span>, Xin Jin, Boyuan Sun, Chun-Le Guo, Ming-Ming Cheng#. <br/>
                <span class="italic"><a href="https://cvpr.thecvf.com/Conferences/2025">CVPR</a></span>, 2025, (<span style="color: #f09228;">Highlight</span>) <img style="position: relative; top:6px;" src="https://img.shields.io/github/stars/lzyhha/AODRaw?label=%F0%9F%8C%9F%20Star&color=blue">&nbsp;<img style="position: relative; top:6px;" src="https://img.shields.io/github/forks/lzyhha/AODRaw?label=%F0%9F%94%A7%20Fork&color=green"><br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2411.15678">paper</a> / <a class="btn btn-dark" href="https://github.com/lzyhha/AODRaw">code</a> / <a class="btn btn-dark" href="bibtex/li2025towards.html">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix" data-author="Zhong-Yu Li" data-date="202503">
            <div class="row-media" style="height: 120px; background-image: url(assets/paper_teaser/hssl_teaser.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2310.05108">Enhancing Representations through Heterogeneous Self-Supervised Learning</a><br/>
                <span class="bold">Zhong-Yu Li</span>, Bo-Wen Yin, Yongxiang Liu, Li Liu, Ming-Ming Cheng#. <br/>
                <span class="italic"><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a></span>, 2025, <br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2310.05108">paper</a> / <a class="btn btn-dark" href="https://github.com/NK-JittorCV/Self-Supervised/">code</a> / <a class="btn btn-dark" href="bibtex/li2023enhancing.html">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix" data-author="Zhong-Yu Li" data-date="202503">
            <div class="row-media" style="height: 120px; background-image: url(assets/paper_teaser/sere_teaser.jpg);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2206.05184">SERE: Exploring Feature Self-relation for Self-supervised Transformer</a><br/>
                <span class="bold">Zhong-Yu Li</span>, Shanghua Gao#, Ming-Ming Cheng.<br/>
                <span class="italic"><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a></span>, 2023, <br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2206.05184">paper</a> / <a class="btn btn-dark" href="https://github.com/MCG-NKU/SERE">code</a> / <a class="btn btn-dark" href="bibtex/li2023sere.html">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix" data-author="Yunheng Li" data-date="202406">
            <div class="row-media" style="height: 100px; background-image: url(assets/paper_teaser/cascade_teaser.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2406.00670">Cascade-CLIP: Cascaded Vision-Language Embeddings Alignment for Zero-Shot Semantic Segmentation</a><br/>
                Yunheng Li, <span class="bold">Zhong-Yu Li</span>, Quansheng Zeng, Qibin Hou#, Ming-Ming Cheng.<br/>
                <span class="italic"><a href="https://icml.cc/Conferences/2024">ICML</a></span>, 2024 <br/>
                 <a class="btn btn-red" href="https://arxiv.org/abs/2406.00670">paper</a> / <a class="btn btn-dark" href="https://github.com/HVision-NKU/Cascade-CLIP">code</a> / <a class="btn btn-dark" href="bibtex/li2024cascade.html">bibtex</a>
            </div>
        </div>
        <div class="publication row clearfix" data-author="Shanghua Gao" data-date="202306">
            <div class="row-media" style="height: 120px; background-image: url(assets/paper_teaser/luss_teaser.jpg);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://lusseg.github.io/">Large-scale Unsupervised Semantic Segmentation</a><br/>
                Shanghua Gao, <span class="bold">Zhong-Yu Li</span>, Ming-Hsuan Yang, Ming-Ming Cheng#, Junwei Han, Philip Torr.<br/>
                <span class="italic"><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a></span>, 2023 <br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2106.03149">paper</a> / <a class="btn btn-dark" href="https://github.com/LUSSeg">code</a> / <a class="btn btn-dark" href="bibtex/gao2022luss.html">bibtex</a>
            </div>
        </div>
        <!-- DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation -->
        <div class="publication row clearfix" data-author="Bo-Wen Yin" data-date="202309">
            <div class="row-media" style="height: 120px; background-image: url(assets/paper_teaser/dformer_teaser.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2309.09668">DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation</a><br/>
                Bo-Wen Yin, Xuying Zhang, <span class="bold">Zhong-Yu Li</span>, Li Liu, Ming-Ming Cheng, Qibin Hou#.<br/>
                <span class="italic"><a href="https://iclr.cc/Conferences/2024">ICLR</a></span>, 2024 <br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2309.09668">paper</a> / <a class="btn btn-dark" href="https://github.com/VCIP-RGBD/DFormer">code</a> / <a class="btn btn-dark" href="bibtex/yin2024dformer.html">bibtex</a>
            </div>
        </div>
        <!-- RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks -->
        <div class="publication row clearfix" data-author="Shanghua Gao" data-date="202206">
            <div class="row-media" style="height: 120px; background-image: url(assets/paper_teaser/rf_teaser.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2206.06637">RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks</a><br/>
                Shanghua Gao, <span class="bold">Zhong-Yu Li</span>, Qi Han, Ming-Ming Cheng#, Liang Wang.<br/>
                <span class="italic"><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a></span>, 2023 <br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2206.06637">paper</a> / <a class="btn btn-dark" href="https://github.com/ShangHua-Gao/RFNext">code</a> / <a class="btn btn-dark" href="bibtex/gao2022rf.html">bibtex</a>
            </div>
        </div>
        <!-- RF-Next: Efficient Receptive Field Search for Convolutional Neural Networks -->
        <div class="publication row clearfix" data-author="Shanghua Gao" data-date="202206">
            <div class="row-media" style="height: 120px; background-image: url(assets/paper_teaser/gl_teaser.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2206.06637">Global2local: Efficient structure search for video action segmentation</a><br/>
                Shang-Hua Gao, Qi Han, <span class="bold">Zhong-Yu Li</span>, Pai Peng, Liang Wang, Ming-Ming Cheng#.<br/>
                <span class="italic"><a href="https://cvpr2021.thecvf.com/">CVPR</a></span>, 2021 <br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2206.06637">paper</a> / <a class="btn btn-dark" href="https://github.com/ShangHua-Gao/RFNext">code</a> / <a class="btn btn-dark" href="bibtex/gao2021global2local.html">bibtex</a>
            </div>
        </div>
        <!-- Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT -->
        <div class="publication row clearfix" data-author="Dongyang Liu" data-date="202502">
            <div class="row-media" style="height: 120px; background-image: url(assets/paper_teaser/luminavideo_teaser.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2502.06781">Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT</a><br/>
                Dongyang Liu*, Shicheng Li*, Yutong Liu*, Zhen Li*, Kai Wang*, Xinyue Li*, Qi Qin, Yufei Liu, Yi Xin, <span class="bold">Zhong-Yu Li</span>, Bin Fu, Chenyang Si, Yuewen Cao, Conghui He, Ziwei Liu, Yu Qiao, Qibin Hou, Hongsheng Li#, Peng Gao#<br/>
                <span class="italic">arxiv</span>, 2025 <br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2502.06782">paper</a> / <a class="btn btn-red" href="https://github.com/Alpha-VLLM/Lumina-Video">code</a> / <a class="btn btn-dark" href="bibtex/liu2025lumina.html">bibtex</a>
            </div>
        </div>
        <!-- IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models -->
        <div class="publication row clearfix" data-author="Jiayi Lei" data-date="202501">
            <div class="row-media" style="height: 120px; background-image: url(assets/paper_teaser/imagine_teaser.jpg);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/abs/2501.13920">IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models</a><br/>
                Jiayi Lei*, Renrui Zhang*, Xiangfei Hu, Weifeng Lin, Zhen Li, Wenjian Sun, Ruoyi Du, Le Zhuo, <span class="bold">Zhong-Yu Li</span>, Xinyue Li, Shitian Zhao, Ziyu Guo, Yiting Lu, Peng Gao#, Hongsheng Li#<br/>
                <span class="italic">arXiv</span>, 2025 <br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2501.13920">paper</a> / <a class="btn btn-dark" href="https://github.com/jylei16/Imagine-e">code</a> / <a class="btn btn-dark" href="bibtex/lei2025imaginee.html">bibtex</a>
            </div>
        </div>
    </div>

    <!-- Experience -->
    <div id="experiences">
        <h2 class="noselect">Experience</h2>
        <div class="timeline noselect">
            <div class="timeline-item">
                <table>
                    <tr>
                        <td class="icon">
                            <a href="https://www.shlab.org.cn/">
                            <img style="width: 125px;" src="assets/experience/pjlab.png" alt="PJLab"></a><br>
                            <span style="font-size: 13px;">2024/12 - Now <br> Shanghai, China</span>
                        </td>
                        <td class="description">
                            <h4><b>Research Intern</b></h4>
                            <p style="font-size: 13px;">Working with <a href="https://scholar.google.com/citations?user=_go6DPsAAAAJ">Peng Gao</a> and <a href="https://paper99.github.io/">Zhen Li</a>, where I completed the project of <a href="https://visualcloze.github.io/">VisualCloze</a>, a universal image generation framework with visual in-context learning. It supports a wide range of in-domain tasks, and generalizes to unseen tasks.</p>
                        </td>
                    </tr>
                </table>
            </div>
        </div>
    </div>

    <!-- Education -->
    <div>
        <h2 class="noselect">Education</h2>
        <div  class="noselect">
            <span><strong>2021/09 - Present</strong>, I am a Ph.D student at <a href="https://cc.nankai.edu.cn/">College of Computer Science, Nankai University</a>, under the supervision of Prof. <a href="https://mmcheng.net/">Ming-Ming Cheng</a>.</span>
            <br/><br/>
            <span><strong>2017/09 - 2021/07</strong>, I was an undergraduate student at <a href="https://cc.nankai.edu.cn/">College of Computer Science and Technology, Nankai University</a>.</span>
        </div>
    </div>

    <!-- Contact -->
    <div class="noselect">
        <a id="contact"></a>
        <h2>Contact</h2>
        You are very welcome to contact me. 
        I can be contacted directly at <span class="bold">lizhongyu</span> [at] <span class="bold">mail.nankai.edu.cn</span>.
    </div>
</div>

<div id="footer-clusrmaps" style="text-align: center; margin-bottom: 20px;">
    <div id="clustrmaps-widget" style="width: 100px; margin: 0 auto; overflow: hidden;">
	<script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=DNH8yFEg2EOUeMN2QQDJLPeeRDJWaiFkxH3u-dUqvV8"></script>
    </div>
</div>

<div class="footer noselect">
    <div class="footer-content">
        &copy; 2025 Zhong-Yu Li. This template is now maintained by <a href="https://github.com/lzyhha/lzyhha.github.io">me</a>, originally template from <a style="color: white; text-decoration: underline;" href="https://nicklashansen.github.io">here</a>.
    </div>
</div>

<style>
.particle-text {
    position: relative;
    cursor: pointer;
    display: inline-block;
}

.particle-text .hover-content {
    display: none;
    position: absolute;
    left: 0;
    top: 0;
    width: max-content;
    white-space: nowrap;
}

.particle-text:hover .hover-content {
    display: inline-block;
}

.particle-text .particle-placeholder {
    visibility: visible;
}

.particle-text:hover .particle-placeholder {
    visibility: hidden;
}

.particle-canvas {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    pointer-events: none;
    z-index: 10;
    opacity: 0.7;
}

.particle-text:hover .particle-canvas {
    display: none;
}
</style>

<script>
function sortPublications(criteria) {
    const container = document.getElementById('publications');
    const publications = Array.from(container.getElementsByClassName('publication'));

    publications.sort((a, b) => {
        if (criteria === 'author') {
            if (a.getAttribute('data-author') === 'Zhong-Yu Li' && b.getAttribute('data-author') !== 'Zhong-Yu Li') {
                return -1;
            } else if (a.getAttribute('data-author') !== 'Zhong-Yu Li' && b.getAttribute('data-author') === 'Zhong-Yu Li') {
                return 1;
            } else {
                return b.getAttribute('data-date') - a.getAttribute('data-date');
            }
        } else if (criteria === 'date') {
            return b.getAttribute('data-date') - a.getAttribute('data-date');
        }
    });

    publications.forEach(pub => {
        container.appendChild(pub);
        // Add gray overlay to non-first-author publications
        // if (criteria === 'author' && pub.getAttribute('data-author') !== 'Zhong-Yu Li') {
        //     pub.classList.add('gray-overlay');
        // } else {
        //     pub.classList.remove('gray-overlay');
        // }
    });

    // Toggle active class
    document.getElementById('sort-author').classList.toggle('active', criteria === 'author');
    document.getElementById('sort-date').classList.toggle('active', criteria === 'date');
}

document.addEventListener('DOMContentLoaded', function() {
    sortPublications('author');
});

// Particle effect for text
document.addEventListener('DOMContentLoaded', function() {
    const particleTexts = document.querySelectorAll('.particle-text');
    
    particleTexts.forEach(text => {
        // 保存原始内容并测量其宽度
        const originalContent = text.innerHTML;
        
        // 创建一个临时元素来测量文本宽度
        const tempSpan = document.createElement('span');
        tempSpan.style.visibility = 'hidden';
        tempSpan.style.position = 'absolute';
        tempSpan.style.whiteSpace = 'nowrap';
        tempSpan.innerHTML = originalContent;
        document.body.appendChild(tempSpan);
        
        // 获取文本宽度
        const textWidth = tempSpan.offsetWidth;
        const textHeight = tempSpan.offsetHeight;
        
        // 移除临时元素
        document.body.removeChild(tempSpan);
        
        // 将原始内容包装到hover-content中，并添加样式确保宽度一致
        const placeholderSpan = document.createElement('span');
        placeholderSpan.className = 'particle-placeholder';
        placeholderSpan.style.display = 'inline-block';
        placeholderSpan.style.width = textWidth + 'px';
        placeholderSpan.innerHTML = '.'.repeat(1);
        
        const hoverSpan = document.createElement('span');
        hoverSpan.className = 'hover-content';
        hoverSpan.style.whiteSpace = 'nowrap';
        hoverSpan.innerHTML = originalContent;
        
        text.innerHTML = '';
        text.appendChild(placeholderSpan);
        text.appendChild(hoverSpan);
        text.style.width = textWidth + 'px';
        text.style.display = 'inline-block';
        
        // Create a canvas element for each particle text
        const canvas = document.createElement('canvas');
        canvas.className = 'particle-canvas';
        canvas.style.width = textWidth + 'px';  // 设置canvas宽度与文本相同
        text.appendChild(canvas);
        
        // Initialize particle system immediately instead of on hover
        const ctx = canvas.getContext('2d');
        const particles = [];
        let animationFrame;
        
        // Set canvas size
        const resizeCanvas = () => {
            canvas.width = textWidth;
            canvas.height = Math.max(textHeight, 20); // 确保高度足够
        };
        resizeCanvas();
        
        // Particle class
        class Particle {
            constructor() {
                this.x = Math.random() * canvas.width;
                this.y = Math.random() * canvas.height;
                this.size = Math.random() * 3 + 1;
                this.speedX = Math.random() * 2 - 1;
                this.speedY = Math.random() * 2 - 1;
                this.color = `hsl(${Math.random() * 60 + 180}, 100%, 50%)`;
                // this.color = `hsl(0, 0%, ${Math.random() * 100}%)`;
            }
            
            update() {
                this.x += this.speedX;
                this.y += this.speedY;
                
                // 边界检查，让粒子保持在文本区域内
                if (this.x < 0 || this.x > canvas.width) {
                    this.speedX = -this.speedX;
                }
                if (this.y < 0 || this.y > canvas.height) {
                    this.speedY = -this.speedY;
                }
                
                if (this.size > 0.2) this.size -= 0.05;
            }
            
            draw() {
                ctx.fillStyle = this.color;
                ctx.beginPath();
                ctx.arc(this.x, this.y, this.size, 0, Math.PI * 2);
                ctx.fill();
            }
        }
        
        // Create particles
        const createParticles = () => {
            // 根据文本宽度调整粒子数量
            const particleCount = Math.max(1, Math.floor(textWidth / 30));
            for (let i = 0; i < particleCount; i++) {
                particles.push(new Particle());
            }
        };
        
        // Animation loop
        const animate = () => {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            createParticles();
            
            for (let i = 0; i < particles.length; i++) {
                particles[i].update();
                particles[i].draw();
                
                if (particles[i].size <= 0.2) {
                    particles.splice(i, 1);
                    i--;
                }
            }
            
            animationFrame = requestAnimationFrame(animate);
        };
        
        // Start animation immediately
        animate();
        
        // Pause animation on hover
        text.addEventListener('mouseenter', () => {
            cancelAnimationFrame(animationFrame);
        });
        
        // Resume animation when not hovering
        text.addEventListener('mouseleave', () => {
            animate();
        });
    });
});
</script>
